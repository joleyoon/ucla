{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Homework 3 (Python)\n",
        "Converted from the original RMarkdown to a Python notebook using torch for modeling.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train shape: (11200, 785), Test shape: (2400, 785), Val shape: (2400, 785)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "from pathlib import Path\n",
        "\n",
        "# Reproducibility\n",
        "torch.manual_seed(0)\n",
        "rng = np.random.default_rng(0)\n",
        "\n",
        "data_dir = Path('.')\n",
        "train_df = pd.read_csv(data_dir / 'mnist_train.csv')\n",
        "test_df = pd.read_csv(data_dir / 'mnist_test.csv')\n",
        "val_df = pd.read_csv(data_dir / 'mnist_val.csv')\n",
        "\n",
        "print(f\"Train shape: {train_df.shape}, Test shape: {test_df.shape}, Val shape: {val_df.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def filter_digits(df, digit_a, digit_b):\n",
        "    filtered = df[df['label'].isin([digit_a, digit_b])].copy()\n",
        "    x = filtered.drop(columns=['label']).to_numpy(dtype=np.float32)\n",
        "    y = (filtered['label'] == digit_b).astype(np.float32).to_numpy()\n",
        "    x_tensor = torch.tensor(x)\n",
        "    y_tensor = torch.tensor(y).unsqueeze(1)\n",
        "    return x_tensor, y_tensor\n",
        "\n",
        "\n",
        "class TwoLayerMLP(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, 1)\n",
        "        self.act = nn.ReLU()\n",
        "        self.out = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.act(self.fc1(x))\n",
        "        x = self.out(self.fc2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "def run_experiment(digit_a, digit_b, epochs=30, batch_sizes=(64, 128, 256), hidden_sizes=(64, 128, 256), base_lr=0.1, decay_rate=0.7):\n",
        "    train_x, train_y = filter_digits(train_df, digit_a, digit_b)\n",
        "    val_x, val_y = filter_digits(val_df, digit_a, digit_b)\n",
        "    test_x, test_y = filter_digits(test_df, digit_a, digit_b)\n",
        "\n",
        "    input_dim = train_x.shape[1]\n",
        "    loss_fn = nn.BCELoss()\n",
        "    history = {}\n",
        "    best_val = float('inf')\n",
        "    best_state = None\n",
        "    best_config = None\n",
        "\n",
        "    for batch_size in batch_sizes:\n",
        "        for hidden_dim in hidden_sizes:\n",
        "            model = TwoLayerMLP(input_dim, hidden_dim)\n",
        "            optimizer = torch.optim.SGD(model.parameters(), lr=base_lr)\n",
        "\n",
        "            train_loss_history = []\n",
        "            val_loss_history = []\n",
        "\n",
        "            for epoch in range(epochs):\n",
        "                lr = base_lr * (decay_rate ** epoch)\n",
        "                for group in optimizer.param_groups:\n",
        "                    group['lr'] = lr\n",
        "\n",
        "                perm = torch.randperm(train_x.shape[0])\n",
        "                for start in range(0, train_x.shape[0], batch_size):\n",
        "                    idx = perm[start:start + batch_size]\n",
        "                    x_batch = train_x[idx]\n",
        "                    y_batch = train_y[idx]\n",
        "\n",
        "                    optimizer.zero_grad()\n",
        "                    preds = model(x_batch)\n",
        "                    loss = loss_fn(preds, y_batch)\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    train_pred = model(train_x)\n",
        "                    val_pred = model(val_x)\n",
        "                    train_loss = loss_fn(train_pred, train_y).item()\n",
        "                    val_loss = loss_fn(val_pred, val_y).item()\n",
        "\n",
        "                train_loss_history.append(train_loss)\n",
        "                val_loss_history.append(val_loss)\n",
        "\n",
        "            key = f\"Batch_{batch_size}_Hidden_{hidden_dim}\"\n",
        "            history[key] = {\"train\": train_loss_history, \"val\": val_loss_history}\n",
        "\n",
        "            if min(val_loss_history) < best_val:\n",
        "                best_val = min(val_loss_history)\n",
        "                best_state = {k: v.clone() for k, v in model.state_dict().items()}\n",
        "                best_config = key\n",
        "\n",
        "    # Evaluate best model on test set\n",
        "    best_hidden = int(best_config.split('_')[-1])\n",
        "    best_model = TwoLayerMLP(input_dim, best_hidden)\n",
        "    best_model.load_state_dict(best_state)\n",
        "    best_model.eval()\n",
        "    with torch.no_grad():\n",
        "        test_probs = best_model(test_x).squeeze().numpy()\n",
        "        test_labels = test_y.squeeze().numpy()\n",
        "        auc = roc_auc_score(test_labels, test_probs)\n",
        "        fpr, tpr, _ = roc_curve(test_labels, test_probs)\n",
        "\n",
        "    return history, best_config, auc, (fpr, tpr)\n",
        "\n",
        "\n",
        "def plot_losses(history, title):\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    for key, series in history.items():\n",
        "        plt.plot(range(1, len(series['train']) + 1), series['train'], label=f\"{key} train\")\n",
        "        plt.plot(range(1, len(series['val']) + 1), series['val'], linestyle='--', label=f\"{key} val\")\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_roc(roc_data, label):\n",
        "    fpr, tpr = roc_data\n",
        "    plt.plot(fpr, tpr, label=label)\n",
        "    plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('ROC Curve')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run experiments for digit pairs\n",
        "history_35, best_35, auc_35, roc_35 = run_experiment(3, 5)\n",
        "history_49, best_49, auc_49, roc_49 = run_experiment(4, 9)\n",
        "\n",
        "print(f\"Best config 3 vs 5: {best_35}, Test AUC: {auc_35:.4f}\")\n",
        "print(f\"Best config 4 vs 9: {best_49}, Test AUC: {auc_49:.4f}\")\n",
        "\n",
        "plot_losses(history_35, 'Training vs Validation Loss (3 vs 5)')\n",
        "plot_losses(history_49, 'Training vs Validation Loss (4 vs 9)')\n",
        "plot_roc(roc_35, 'Digits 3 vs 5')\n",
        "plot_roc(roc_49, 'Digits 4 vs 9')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LogisticClassifier(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Linear(input_dim, 1)\n",
        "        self.out = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.out(self.fc(x))\n",
        "\n",
        "\n",
        "def train_and_eval_logistic(digit_a, digit_b, epochs=30, base_lr=0.1, decay_rate=0.7):\n",
        "    train_x, train_y = filter_digits(train_df, digit_a, digit_b)\n",
        "    test_x, test_y = filter_digits(test_df, digit_a, digit_b)\n",
        "\n",
        "    input_dim = train_x.shape[1]\n",
        "    model = LogisticClassifier(input_dim)\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=base_lr)\n",
        "    loss_fn = nn.BCELoss()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        lr = base_lr * (decay_rate ** epoch)\n",
        "        for group in optimizer.param_groups:\n",
        "            group['lr'] = lr\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(train_x)\n",
        "        loss = loss_fn(preds, train_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        test_probs = model(test_x).squeeze().numpy()\n",
        "        test_labels = test_y.squeeze().numpy()\n",
        "        auc = roc_auc_score(test_labels, test_probs)\n",
        "        fpr, tpr, _ = roc_curve(test_labels, test_probs)\n",
        "    return auc, (fpr, tpr)\n",
        "\n",
        "\n",
        "a_35, roc_log_35 = train_and_eval_logistic(3, 5)\n",
        "a_49, roc_log_49 = train_and_eval_logistic(4, 9)\n",
        "\n",
        "print(f\"Logistic regression AUC (3 vs 5): {a_35:.4f}\")\n",
        "print(f\"Logistic regression AUC (4 vs 9): {a_49:.4f}\")\n",
        "\n",
        "plot_roc(roc_log_35, 'Logistic: 3 vs 5')\n",
        "plot_roc(roc_log_49, 'Logistic: 4 vs 9')\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
