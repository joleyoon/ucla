{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 102B Final Project Part B (Python)\n\n",
        "Standalone version that recomputes Part A results needed for Part B.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "# Recompute Part A results so this notebook can run standalone.\n",
        "\n",
        "def soft_threshold(z, lambda_val):\n",
        "    return np.sign(z) * np.maximum(np.abs(z) - lambda_val, 0)\n",
        "\n",
        "\n",
        "def cd_lasso(X, y, lambda_val, tol=1e-4, max_iter=200):\n",
        "    n_obs, n_feat = X.shape\n",
        "    beta_vec = np.zeros(n_feat)\n",
        "    for _ in range(max_iter):\n",
        "        beta_prev = beta_vec.copy()\n",
        "        for j in range(n_feat):\n",
        "            r_j = y - X @ beta_vec + X[:, j] * beta_vec[j]\n",
        "            rho_j = np.sum(X[:, j] * r_j) / n_obs\n",
        "            denom_j = np.sum(X[:, j] ** 2) / n_obs\n",
        "            beta_vec[j] = soft_threshold(rho_j, lambda_val) / denom_j\n",
        "        diff_norm = np.sqrt(np.sum((beta_vec - beta_prev) ** 2))\n",
        "        if diff_norm < tol:\n",
        "            break\n",
        "    return beta_vec\n",
        "\n",
        "\n",
        "def make_lambda_grid(X, y, grid_size=10):\n",
        "    X_centered = X - X.mean(axis=0, keepdims=True)\n",
        "    y_centered = y - y.mean()\n",
        "    n = X.shape[0]\n",
        "    lambda_max = np.max(np.abs(X_centered.T @ y_centered)) / n\n",
        "    lambda_seq = np.exp(np.linspace(np.log(lambda_max), np.log(lambda_max * 1e-3), grid_size))\n",
        "    return lambda_seq\n",
        "\n",
        "\n",
        "def fit_and_eval(file_train, file_test, grid_size=10):\n",
        "    df = pd.read_csv(file_train)\n",
        "    X_all = df.iloc[:, :600].to_numpy()\n",
        "    y_all = df.iloc[:, 600].to_numpy(dtype=float)\n",
        "\n",
        "    n_total = X_all.shape[0]\n",
        "    split_index = int(np.floor(0.8 * n_total))\n",
        "    X_train = X_all[:split_index]\n",
        "    y_train = y_all[:split_index]\n",
        "    X_vali = X_all[split_index:]\n",
        "    y_vali = y_all[split_index:]\n",
        "\n",
        "    X_mean = X_train.mean(axis=0)\n",
        "    y_mean = y_train.mean()\n",
        "    X_train_c = X_train - X_mean\n",
        "    y_train_c = y_train - y_mean\n",
        "    X_vali_c = X_vali - X_mean\n",
        "\n",
        "    lambda_values = make_lambda_grid(X_train, y_train, grid_size)\n",
        "    val_errors = np.zeros(len(lambda_values))\n",
        "\n",
        "    for i, lam in enumerate(lambda_values):\n",
        "        beta_hat = cd_lasso(X_train_c, y_train_c, lam)\n",
        "        intercept = y_mean - np.sum(beta_hat * X_mean)\n",
        "        y_pred = X_vali @ beta_hat + intercept\n",
        "        val_errors[i] = np.mean((y_vali - y_pred) ** 2)\n",
        "\n",
        "    best_idx = int(np.argmin(val_errors))\n",
        "    best_lambda = lambda_values[best_idx]\n",
        "\n",
        "    X_combined = np.vstack([X_train, X_vali])\n",
        "    y_combined = np.concatenate([y_train, y_vali])\n",
        "    X_comb_mean = X_combined.mean(axis=0)\n",
        "    y_comb_mean = y_combined.mean()\n",
        "    X_comb_c = X_combined - X_comb_mean\n",
        "    y_comb_c = y_combined - y_comb_mean\n",
        "\n",
        "    final_beta = cd_lasso(X_comb_c, y_comb_c, best_lambda)\n",
        "\n",
        "    return {\n",
        "        \"lambda\": best_lambda,\n",
        "        \"nonzero\": np.where(final_beta != 0)[0] + 1,\n",
        "    }\n",
        "\n",
        "\n",
        "# Rebuild results_list from Part A logic.\n",
        "\n",
        "data_dir = Path('.')\n",
        "file_test = data_dir / 'test_data.csv'\n",
        "node_files = [\n",
        "    data_dir / 'regression_data_node1.csv',\n",
        "    data_dir / 'regression_data_node2.csv',\n",
        "    data_dir / 'regression_data_node3.csv',\n",
        "]\n",
        "\n",
        "results_list = [fit_and_eval(f, file_test, grid_size=10) for f in node_files]\n",
        "\n",
        "results_b = [\n",
        "    {\"lam\": res[\"lambda\"], \"nonzero\": res[\"nonzero\"]}\n",
        "    for res in results_list\n",
        "]\n",
        "\n",
        "lambda_vec = np.array([x[\"lam\"] for x in results_b])\n",
        "nonzero_sets = [x[\"nonzero\"] for x in results_b]\n",
        "\n",
        "\n",
        "def soft_thresh(z, lam):\n",
        "    return np.sign(z) * np.maximum(np.abs(z) - lam, 0)\n",
        "\n",
        "\n",
        "def cd_partial(Xmat, yvec, lam, beta_init, rounds=5):\n",
        "    beta = beta_init.copy()\n",
        "    n, p = Xmat.shape\n",
        "\n",
        "    for _ in range(rounds):\n",
        "        for j in range(p):\n",
        "            partial_res = yvec - Xmat @ beta + Xmat[:, j] * beta[j]\n",
        "            rho_j = np.sum(Xmat[:, j] * partial_res) / n\n",
        "            denom = np.sum(Xmat[:, j] ** 2) / n\n",
        "            beta[j] = soft_thresh(rho_j, lam) / denom\n",
        "    return beta\n",
        "\n",
        "\n",
        "p = 600\n",
        "\n",
        "rng = np.random.default_rng(123)\n",
        "node_data = []\n",
        "for k in range(1, 4):\n",
        "    df = pd.read_csv(data_dir / f\"regression_data_node{k}.csv\")\n",
        "    X = df.iloc[:, :p].to_numpy()\n",
        "    y = df.iloc[:, p].to_numpy(dtype=float)\n",
        "\n",
        "    idx = rng.choice(X.shape[0], size=int(np.floor(0.8 * X.shape[0])), replace=False)\n",
        "    node_data.append({\n",
        "        \"Xtrain\": X[idx],\n",
        "        \"ytrain\": y[idx],\n",
        "        \"size\": len(idx),\n",
        "    })\n",
        "\n",
        "sample_weights = np.array([d[\"size\"] for d in node_data])\n",
        "\n",
        "# Federated Learning (5 iterations/round)\n",
        "\n",
        "beta_global = np.zeros(p)\n",
        "converge_tol = 1e-6\n",
        "\n",
        "while True:\n",
        "    beta_prev = beta_global.copy()\n",
        "\n",
        "    local_models = np.stack([\n",
        "        cd_partial(\n",
        "            d[\"Xtrain\"], d[\"ytrain\"],\n",
        "            lam=lambda_vec[k],\n",
        "            beta_init=beta_global,\n",
        "            rounds=5,\n",
        "        )\n",
        "        for k, d in enumerate(node_data)\n",
        "    ], axis=1)\n",
        "\n",
        "    beta_global = (local_models * sample_weights).sum(axis=1) / sample_weights.sum()\n",
        "\n",
        "    if np.sqrt(np.sum((beta_global - beta_prev) ** 2)) < converge_tol:\n",
        "        break\n",
        "\n",
        "agg_beta_5 = beta_global\n",
        "agg_nonzero_5 = np.where(agg_beta_5 != 0)[0] + 1\n",
        "\n",
        "print(\"Non-zero coefficients:\", \", \".join(str(i) for i in agg_nonzero_5))\n",
        "\n",
        "# Confusion Matrix\n",
        "for k in range(1, 4):\n",
        "    truth_vec = np.isin(np.arange(1, p + 1), agg_nonzero_5).astype(int)\n",
        "    pred_vec = np.isin(np.arange(1, p + 1), nonzero_sets[k - 1]).astype(int)\n",
        "\n",
        "    tp = np.sum((truth_vec == 1) & (pred_vec == 1))\n",
        "    fp = np.sum((truth_vec == 0) & (pred_vec == 1))\n",
        "    fn = np.sum((truth_vec == 1) & (pred_vec == 0))\n",
        "    tn = np.sum((truth_vec == 0) & (pred_vec == 0))\n",
        "\n",
        "    conf_mat = pd.DataFrame(\n",
        "        {\n",
        "            \"Pred_0\": [tn, fn],\n",
        "            \"Pred_1\": [fp, tp],\n",
        "        },\n",
        "        index=[\"True_0\", \"True_1\"],\n",
        "    )\n",
        "\n",
        "    print(f\"\n",
        "Confusion Matrix (5 iterations, Node {k}):\")\n",
        "    print(conf_mat)\n",
        "\n",
        "# Case 1: 5 iterations per node\n",
        "\n",
        "df_test = pd.read_csv(data_dir / \"test_data.csv\")\n",
        "Xtest = df_test.iloc[:, :p].to_numpy()\n",
        "ytest = df_test.iloc[:, p].to_numpy(dtype=float)\n",
        "\n",
        "test_loss_5 = np.mean((Xtest @ agg_beta_5 - ytest) ** 2)\n",
        "print(f\"\n",
        "Test Loss (5 iterations): {test_loss_5:.6f}\")\n",
        "\n",
        "# Case 2: 10 iterations per node\n",
        "\n",
        "beta_global = np.zeros(p)\n",
        "\n",
        "while True:\n",
        "    beta_prev = beta_global.copy()\n",
        "\n",
        "    local_models = np.stack([\n",
        "        cd_partial(\n",
        "            d[\"Xtrain\"], d[\"ytrain\"],\n",
        "            lam=lambda_vec[k],\n",
        "            beta_init=beta_global,\n",
        "            rounds=10,\n",
        "        )\n",
        "        for k, d in enumerate(node_data)\n",
        "    ], axis=1)\n",
        "    beta_global = (local_models * sample_weights).sum(axis=1) / sample_weights.sum()\n",
        "    if np.sqrt(np.sum((beta_global - beta_prev) ** 2)) < converge_tol:\n",
        "        break\n",
        "\n",
        "agg_beta_10 = beta_global\n",
        "agg_nonzero_10 = np.where(agg_beta_10 != 0)[0] + 1\n",
        "\n",
        "print(\", \".join(str(i) for i in agg_nonzero_10))\n",
        "\n",
        "test_loss_10 = np.mean((Xtest @ agg_beta_10 - ytest) ** 2)\n",
        "print(f\"Test Loss (10 iterations): {test_loss_10:.6f}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}