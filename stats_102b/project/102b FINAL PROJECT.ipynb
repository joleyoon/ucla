{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 102b Final Project (Python)\n",
        "Converted from the original RMarkdown to a Python notebook.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "\n",
        "# Function to perform LASSO regression using Coordinate Descent\n",
        "\n",
        "def lasso_cd(response, features, coeffs_init=None, max_iter=1000, epsilon=1e-6, reg=0.1):\n",
        "    n_samples, n_features = features.shape\n",
        "\n",
        "    if coeffs_init is None:\n",
        "        coeffs = np.zeros(n_features)\n",
        "    else:\n",
        "        coeffs = coeffs_init.copy()\n",
        "\n",
        "    def shrink(z, gamma):\n",
        "        return np.sign(z) * np.maximum(np.abs(z) - gamma, 0)\n",
        "\n",
        "    col_norms = np.sum(features ** 2, axis=0)\n",
        "    resid = response - features @ coeffs\n",
        "\n",
        "    for _ in range(max_iter):\n",
        "        prev_coeffs = coeffs.copy()\n",
        "        for k in range(n_features):\n",
        "            feature_k = features[:, k]\n",
        "            partial_resid = resid + coeffs[k] * feature_k\n",
        "            update_val = feature_k @ partial_resid\n",
        "            coeffs[k] = shrink(update_val, reg) / col_norms[k]\n",
        "            resid = partial_resid - coeffs[k] * feature_k\n",
        "\n",
        "        if np.sqrt(np.sum((coeffs - prev_coeffs) ** 2)) < epsilon:\n",
        "            break\n",
        "\n",
        "    return coeffs\n",
        "\n",
        "\n",
        "# Load all datasets\n",
        "\n",
        "data_dir = Path('.')\n",
        "dataset1 = pd.read_csv(data_dir / \"regression_data_node1.csv\")\n",
        "dataset2 = pd.read_csv(data_dir / \"regression_data_node2.csv\")\n",
        "dataset3 = pd.read_csv(data_dir / \"regression_data_node3.csv\")\n",
        "dataset_test = pd.read_csv(data_dir / \"test_data.csv\")\n",
        "\n",
        "# Split datasets into training (80%) and validation (20%)\n",
        "\n",
        "def split_data(data, split_index):\n",
        "    train = data.iloc[:split_index]\n",
        "    valid = data.iloc[split_index:]\n",
        "    return {\"train\": train, \"valid\": valid}\n",
        "\n",
        "\n",
        "d1 = split_data(dataset1, 160)\n",
        "d2 = split_data(dataset2, 240)\n",
        "d3 = split_data(dataset3, 400)\n",
        "\n",
        "# Extract test set components\n",
        "\n",
        "y_test = dataset_test[\"y\"].to_numpy(dtype=float)\n",
        "X_test = dataset_test.drop(columns=[\"y\"]).to_numpy()\n",
        "\n",
        "# Prepare training and validation sets (features and targets)\n",
        "\n",
        "def extract_xy(df):\n",
        "    y = df[\"y\"].to_numpy(dtype=float)\n",
        "    X = df.drop(columns=[\"y\"]).to_numpy()\n",
        "    return {\"X\": X, \"y\": y}\n",
        "\n",
        "\n",
        "data_all = [\n",
        "    {\"train\": extract_xy(d1[\"train\"]), \"val\": extract_xy(d1[\"valid\"])},\n",
        "    {\"train\": extract_xy(d2[\"train\"]), \"val\": extract_xy(d2[\"valid\"])},\n",
        "    {\"train\": extract_xy(d3[\"train\"]), \"val\": extract_xy(d3[\"valid\"])},\n",
        "]\n",
        "\n",
        "# Define list of candidate lambda values to search\n",
        "\n",
        "lambda_values = np.arange(0.05, 1.01, 0.05)\n",
        "optimal_lambdas = np.zeros(3)\n",
        "val_errors = [\n",
        "    np.zeros(len(lambda_values)),\n",
        "    np.zeros(len(lambda_values)),\n",
        "    np.zeros(len(lambda_values)),\n",
        "]\n",
        "\n",
        "# Model training and lambda tuning for each dataset\n",
        "\n",
        "for d in range(3):\n",
        "    lowest_mse = np.inf\n",
        "    for l, lam in enumerate(lambda_values):\n",
        "        fitted_model = lasso_cd(\n",
        "            response=data_all[d][\"train\"][\"y\"],\n",
        "            features=data_all[d][\"train\"][\"X\"],\n",
        "            reg=lam,\n",
        "        )\n",
        "\n",
        "        predictions = data_all[d][\"val\"][\"X\"] @ fitted_model\n",
        "        mse_val = np.sum((data_all[d][\"val\"][\"y\"] - predictions) ** 2) / (\n",
        "            2 * data_all[d][\"val\"][\"X\"].shape[0]\n",
        "        )\n",
        "        val_errors[d][l] = mse_val\n",
        "\n",
        "        if mse_val < lowest_mse:\n",
        "            lowest_mse = mse_val\n",
        "            optimal_lambdas[d] = lam\n",
        "\n",
        "    print(f\"Best lambda for dataset {d + 1}: {optimal_lambdas[d]}\")\n",
        "\n",
        "# Train final models with best lambda values\n",
        "\n",
        "model1 = lasso_cd(data_all[0][\"train\"][\"y\"], data_all[0][\"train\"][\"X\"], reg=optimal_lambdas[0])\n",
        "model2 = lasso_cd(data_all[1][\"train\"][\"y\"], data_all[1][\"train\"][\"X\"], reg=optimal_lambdas[1])\n",
        "model3 = lasso_cd(data_all[2][\"train\"][\"y\"], data_all[2][\"train\"][\"X\"], reg=optimal_lambdas[2])\n",
        "\n",
        "# Plot validation losses\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(lambda_values, val_errors[0], marker='o')\n",
        "plt.title(\"Validation Loss - Dataset 1\")\n",
        "plt.xlabel(\"Lambda\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.show()\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(lambda_values, val_errors[1], marker='o')\n",
        "plt.title(\"Validation Loss - Dataset 2\")\n",
        "plt.xlabel(\"Lambda\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.show()\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(lambda_values, val_errors[2], marker='o')\n",
        "plt.title(\"Validation Loss - Dataset 3\")\n",
        "plt.xlabel(\"Lambda\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.show()\n",
        "\n",
        "# Identify non-zero coefficients\n",
        "\n",
        "nz1 = np.where(model1 != 0)[0] + 1\n",
        "nz2 = np.where(model2 != 0)[0] + 1\n",
        "nz3 = np.where(model3 != 0)[0] + 1\n",
        "\n",
        "nz1\n",
        "nz2\n",
        "nz3\n",
        "\n",
        "# Common indices across all three models\n",
        "\n",
        "intersect_all = sorted(set(nz1).intersection(nz2, nz3))\n",
        "intersect_all\n",
        "\n",
        "# Calculate test losses\n",
        "\n",
        "def test_loss(X, y, coeffs):\n",
        "    return np.sum((y - X @ coeffs) ** 2) / (2 * X.shape[0])\n",
        "\n",
        "\n",
        "loss1 = test_loss(X_test, y_test, model1)\n",
        "loss2 = test_loss(X_test, y_test, model2)\n",
        "loss3 = test_loss(X_test, y_test, model3)\n",
        "\n",
        "loss1\n",
        "loss2\n",
        "loss3\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Part B (standalone) - recompute required Part A outputs\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "# Recompute minimal Part A outputs needed for Part B.\n",
        "\n",
        "def soft_threshold(z, lambda_val):\n",
        "    return np.sign(z) * np.maximum(np.abs(z) - lambda_val, 0)\n",
        "\n",
        "\n",
        "def cd_lasso(X, y, lambda_val, tol=1e-4, max_iter=200):\n",
        "    n_obs, n_feat = X.shape\n",
        "    beta_vec = np.zeros(n_feat)\n",
        "    for _ in range(max_iter):\n",
        "        beta_prev = beta_vec.copy()\n",
        "        for j in range(n_feat):\n",
        "            r_j = y - X @ beta_vec + X[:, j] * beta_vec[j]\n",
        "            rho_j = np.sum(X[:, j] * r_j) / n_obs\n",
        "            denom_j = np.sum(X[:, j] ** 2) / n_obs\n",
        "            beta_vec[j] = soft_threshold(rho_j, lambda_val) / denom_j\n",
        "        diff_norm = np.sqrt(np.sum((beta_vec - beta_prev) ** 2))\n",
        "        if diff_norm < tol:\n",
        "            break\n",
        "    return beta_vec\n",
        "\n",
        "\n",
        "def make_lambda_grid(X, y, grid_size=10):\n",
        "    X_centered = X - X.mean(axis=0, keepdims=True)\n",
        "    y_centered = y - y.mean()\n",
        "    n = X.shape[0]\n",
        "    lambda_max = np.max(np.abs(X_centered.T @ y_centered)) / n\n",
        "    lambda_seq = np.exp(np.linspace(np.log(lambda_max), np.log(lambda_max * 1e-3), grid_size))\n",
        "    return lambda_seq\n",
        "\n",
        "\n",
        "def fit_and_eval(file_train, grid_size=10):\n",
        "    df = pd.read_csv(file_train)\n",
        "    X_all = df.iloc[:, :600].to_numpy()\n",
        "    y_all = df.iloc[:, 600].to_numpy(dtype=float)\n",
        "\n",
        "    n_total = X_all.shape[0]\n",
        "    split_index = int(np.floor(0.8 * n_total))\n",
        "    X_train = X_all[:split_index]\n",
        "    y_train = y_all[:split_index]\n",
        "    X_vali = X_all[split_index:]\n",
        "    y_vali = y_all[split_index:]\n",
        "\n",
        "    X_mean = X_train.mean(axis=0)\n",
        "    y_mean = y_train.mean()\n",
        "    X_train_c = X_train - X_mean\n",
        "    y_train_c = y_train - y_mean\n",
        "\n",
        "    lambda_values = make_lambda_grid(X_train, y_train, grid_size)\n",
        "    val_errors = np.zeros(len(lambda_values))\n",
        "\n",
        "    for i, lam in enumerate(lambda_values):\n",
        "        beta_hat = cd_lasso(X_train_c, y_train_c, lam)\n",
        "        intercept = y_mean - np.sum(beta_hat * X_mean)\n",
        "        y_pred = X_vali @ beta_hat + intercept\n",
        "        val_errors[i] = np.mean((y_vali - y_pred) ** 2)\n",
        "\n",
        "    best_idx = int(np.argmin(val_errors))\n",
        "    best_lambda = lambda_values[best_idx]\n",
        "\n",
        "    X_combined = np.vstack([X_train, X_vali])\n",
        "    y_combined = np.concatenate([y_train, y_vali])\n",
        "    X_comb_mean = X_combined.mean(axis=0)\n",
        "    y_comb_mean = y_combined.mean()\n",
        "    X_comb_c = X_combined - X_comb_mean\n",
        "    y_comb_c = y_combined - y_comb_mean\n",
        "\n",
        "    final_beta = cd_lasso(X_comb_c, y_comb_c, best_lambda)\n",
        "\n",
        "    return {\n",
        "        \"lambda\": best_lambda,\n",
        "        \"nonzero\": np.where(final_beta != 0)[0] + 1,\n",
        "    }\n",
        "\n",
        "\n",
        "data_dir = Path('.')\n",
        "node_files = [\n",
        "    data_dir / 'regression_data_node1.csv',\n",
        "    data_dir / 'regression_data_node2.csv',\n",
        "    data_dir / 'regression_data_node3.csv',\n",
        "]\n",
        "\n",
        "results_list = [fit_and_eval(f, grid_size=10) for f in node_files]\n",
        "\n",
        "results_b = [\n",
        "    {\"lam\": res[\"lambda\"], \"nonzero\": res[\"nonzero\"]}\n",
        "    for res in results_list\n",
        "]\n",
        "\n",
        "lambda_vec = np.array([x[\"lam\"] for x in results_b])\n",
        "nonzero_sets = [x[\"nonzero\"] for x in results_b]\n",
        "\n",
        "\n",
        "def soft_thresh(z, lam):\n",
        "    return np.sign(z) * np.maximum(np.abs(z) - lam, 0)\n",
        "\n",
        "\n",
        "def cd_partial(Xmat, yvec, lam, beta_init, rounds=5):\n",
        "    beta = beta_init.copy()\n",
        "    n, p = Xmat.shape\n",
        "\n",
        "    for _ in range(rounds):\n",
        "        for j in range(p):\n",
        "            partial_res = yvec - Xmat @ beta + Xmat[:, j] * beta[j]\n",
        "            rho_j = np.sum(Xmat[:, j] * partial_res) / n\n",
        "            denom = np.sum(Xmat[:, j] ** 2) / n\n",
        "            beta[j] = soft_thresh(rho_j, lam) / denom\n",
        "    return beta\n",
        "\n",
        "\n",
        "p = 600\n",
        "\n",
        "rng = np.random.default_rng(123)\n",
        "node_data = []\n",
        "for k in range(1, 4):\n",
        "    df = pd.read_csv(data_dir / f\"regression_data_node{k}.csv\")\n",
        "    X = df.iloc[:, :p].to_numpy()\n",
        "    y = df.iloc[:, p].to_numpy(dtype=float)\n",
        "\n",
        "    idx = rng.choice(X.shape[0], size=int(np.floor(0.8 * X.shape[0])), replace=False)\n",
        "    node_data.append({\n",
        "        \"Xtrain\": X[idx],\n",
        "        \"ytrain\": y[idx],\n",
        "        \"size\": len(idx),\n",
        "    })\n",
        "\n",
        "sample_weights = np.array([d[\"size\"] for d in node_data])\n",
        "\n",
        "# Federated Learning (5 iterations/round)\n",
        "\n",
        "beta_global = np.zeros(p)\n",
        "converge_tol = 1e-6\n",
        "\n",
        "while True:\n",
        "    beta_prev = beta_global.copy()\n",
        "\n",
        "    local_models = np.stack([\n",
        "        cd_partial(\n",
        "            d[\"Xtrain\"], d[\"ytrain\"],\n",
        "            lam=lambda_vec[k],\n",
        "            beta_init=beta_global,\n",
        "            rounds=5,\n",
        "        )\n",
        "        for k, d in enumerate(node_data)\n",
        "    ], axis=1)\n",
        "\n",
        "    beta_global = (local_models * sample_weights).sum(axis=1) / sample_weights.sum()\n",
        "\n",
        "    if np.sqrt(np.sum((beta_global - beta_prev) ** 2)) < converge_tol:\n",
        "        break\n",
        "\n",
        "agg_beta_5 = beta_global\n",
        "agg_nonzero_5 = np.where(agg_beta_5 != 0)[0] + 1\n",
        "\n",
        "print(\"Non-zero coefficients:\", \", \".join(str(i) for i in agg_nonzero_5))\n",
        "\n",
        "# Confusion Matrix\n",
        "for k in range(1, 4):\n",
        "    truth_vec = np.isin(np.arange(1, p + 1), agg_nonzero_5).astype(int)\n",
        "    pred_vec = np.isin(np.arange(1, p + 1), nonzero_sets[k - 1]).astype(int)\n",
        "\n",
        "    tp = np.sum((truth_vec == 1) & (pred_vec == 1))\n",
        "    fp = np.sum((truth_vec == 0) & (pred_vec == 1))\n",
        "    fn = np.sum((truth_vec == 1) & (pred_vec == 0))\n",
        "    tn = np.sum((truth_vec == 0) & (pred_vec == 0))\n",
        "\n",
        "    conf_mat = pd.DataFrame(\n",
        "        {\n",
        "            \"Pred_0\": [tn, fn],\n",
        "            \"Pred_1\": [fp, tp],\n",
        "        },\n",
        "        index=[\"True_0\", \"True_1\"],\n",
        "    )\n",
        "\n",
        "    print(f\"\n",
        "Confusion Matrix (5 iterations, Node {k}):\")\n",
        "    print(conf_mat)\n",
        "\n",
        "# Case 1: 5 iterations per node\n",
        "\n",
        "df_test = pd.read_csv(data_dir / \"test_data.csv\")\n",
        "Xtest = df_test.iloc[:, :p].to_numpy()\n",
        "ytest = df_test.iloc[:, p].to_numpy(dtype=float)\n",
        "\n",
        "test_loss_5 = np.mean((Xtest @ agg_beta_5 - ytest) ** 2)\n",
        "print(f\"\n",
        "Test Loss (5 iterations): {test_loss_5:.6f}\")\n",
        "\n",
        "# Case 2: 10 iterations per node\n",
        "\n",
        "beta_global = np.zeros(p)\n",
        "\n",
        "while True:\n",
        "    beta_prev = beta_global.copy()\n",
        "\n",
        "    local_models = np.stack([\n",
        "        cd_partial(\n",
        "            d[\"Xtrain\"], d[\"ytrain\"],\n",
        "            lam=lambda_vec[k],\n",
        "            beta_init=beta_global,\n",
        "            rounds=10,\n",
        "        )\n",
        "        for k, d in enumerate(node_data)\n",
        "    ], axis=1)\n",
        "    beta_global = (local_models * sample_weights).sum(axis=1) / sample_weights.sum()\n",
        "    if np.sqrt(np.sum((beta_global - beta_prev) ** 2)) < converge_tol:\n",
        "        break\n",
        "\n",
        "agg_beta_10 = beta_global\n",
        "agg_nonzero_10 = np.where(agg_beta_10 != 0)[0] + 1\n",
        "\n",
        "print(\", \".join(str(i) for i in agg_nonzero_10))\n",
        "\n",
        "test_loss_10 = np.mean((Xtest @ agg_beta_10 - ytest) ** 2)\n",
        "print(f\"Test Loss (10 iterations): {test_loss_10:.6f}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}