{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17d9932c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce412a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n, p = 10000 100\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"dataset-logistic-regression.csv\")\n",
    "\n",
    "y = data[\"y\"].astype(float).to_numpy()\n",
    "X = data.drop(columns=[\"y\"]).to_numpy()\n",
    "\n",
    "n, p = X.shape\n",
    "print(\"n, p =\", n, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "921e84eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "def f(beta, X, y):\n",
    "    p = sigmoid(X @ beta)\n",
    "    # clamp to avoid log(0)\n",
    "    p = np.clip(p, 1e-8, 1 - 1e-8)\n",
    "    return -np.sum(y * np.log(p) + (1 - y) * np.log(1 - p))\n",
    "\n",
    "def grad_f(beta, X, y):\n",
    "    p = sigmoid(X @ beta)\n",
    "    return X.T @ (p - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50b4a0be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tail(glm coefficients):\n",
      "[-0.04860354  0.03384919  0.14973405  0.17026521  0.01979437  0.00700259]\n"
     ]
    }
   ],
   "source": [
    "glm_model = sm.Logit(y, X)\n",
    "glm_result = glm_model.fit(disp=False)\n",
    "\n",
    "beta_glm = glm_result.params\n",
    "print(\"tail(glm coefficients):\")\n",
    "print(beta_glm[-6:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cfa2a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/t0/cl4mx7656030fy5dm2t2jwsr0000gn/T/ipykernel_35529/2326215551.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  return 1.0 / (1.0 + np.exp(-z))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tail(gd_back beta):\n",
      "[-0.04860114  0.03385028  0.14973027  0.17026175  0.01979193  0.00700404]\n"
     ]
    }
   ],
   "source": [
    "def gradient_descent_backtracking(X, y, beta_init=None, tol=1e-6, max_iter=10000, epsilon=0.5, tau=0.8):\n",
    "    n, p = X.shape\n",
    "    beta = np.zeros(p) if beta_init is None else beta_init.copy()\n",
    "    eta = 1.0\n",
    "    iterations = 0\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        iterations += 1\n",
    "        grad = grad_f(beta, X, y)\n",
    "        eta_bt = eta\n",
    "        beta_new = beta - eta_bt * grad\n",
    "\n",
    "        while f(beta_new, X, y) > f(beta, X, y) - epsilon * eta_bt * np.sum(grad**2):\n",
    "            eta_bt *= tau\n",
    "            beta_new = beta - eta_bt * grad\n",
    "\n",
    "        if np.linalg.norm(beta_new - beta) < tol:\n",
    "            beta = beta_new\n",
    "            break\n",
    "\n",
    "        beta = beta_new\n",
    "\n",
    "    return {\"beta\": beta, \"iterations\": iterations}\n",
    "\n",
    "gd_back = gradient_descent_backtracking(X, y)\n",
    "print(\"tail(gd_back beta):\")\n",
    "print(gd_back[\"beta\"][-6:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a9a9e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/t0/cl4mx7656030fy5dm2t2jwsr0000gn/T/ipykernel_35529/2326215551.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  return 1.0 / (1.0 + np.exp(-z))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tail(gd_nest beta):\n",
      "[-0.04860551  0.03384797  0.14973736  0.17026769  0.01979698  0.00700183]\n"
     ]
    }
   ],
   "source": [
    "def gradient_descent_nesterov(X, y, beta_init=None, tol=1e-6, max_iter=10000, epsilon=0.5, tau=0.8):\n",
    "    n, p = X.shape\n",
    "    beta = np.zeros(p) if beta_init is None else beta_init.copy()\n",
    "    beta_prev = beta.copy()\n",
    "    eta = 1.0\n",
    "    iterations = 0\n",
    "\n",
    "    for it in range(1, max_iter + 1):\n",
    "        iterations += 1\n",
    "\n",
    "        momentum_step = beta + (it - 1) / (it + 2) * (beta - beta_prev)\n",
    "        grad = grad_f(momentum_step, X, y)\n",
    "\n",
    "        eta_bt = eta\n",
    "        beta_new = momentum_step - eta_bt * grad\n",
    "\n",
    "        while f(beta_new, X, y) > f(momentum_step, X, y) - epsilon * eta_bt * np.sum(grad**2):\n",
    "            eta_bt *= tau\n",
    "            beta_new = momentum_step - eta_bt * grad\n",
    "\n",
    "        if np.linalg.norm(beta_new - beta) < tol:\n",
    "            beta = beta_new\n",
    "            break\n",
    "\n",
    "        beta_prev = beta\n",
    "        beta = beta_new\n",
    "\n",
    "    return {\"beta\": beta, \"iterations\": iterations}\n",
    "\n",
    "gd_nest = gradient_descent_nesterov(X, y)\n",
    "print(\"tail(gd_nest beta):\")\n",
    "print(gd_nest[\"beta\"][-6:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8dbc341e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tail(adam beta):\n",
      "[-0.04860553  0.03384812  0.14973647  0.17026696  0.01979628  0.00700074]\n"
     ]
    }
   ],
   "source": [
    "def amsgrad_adam(X, y, beta_init=None, eta=0.01, beta1=0.9, beta2=0.999, eps=1e-8, max_iter=1000, tol=1e-6):\n",
    "    n, p = X.shape\n",
    "    beta = np.zeros(p) if beta_init is None else beta_init.copy()\n",
    "\n",
    "    m = np.zeros(p)\n",
    "    z = np.full(p, eps)       # second moment\n",
    "    z_hat = np.full(p, eps)   # max past second moments (AMSGrad)\n",
    "    iterations = 0\n",
    "\n",
    "    for t in range(1, max_iter + 1):\n",
    "        iterations += 1\n",
    "        grad = grad_f(beta, X, y)\n",
    "\n",
    "        m = beta1 * m + (1 - beta1) * grad\n",
    "        z = beta2 * z + (1 - beta2) * (grad**2)\n",
    "        z_hat = np.maximum(z_hat, z)\n",
    "\n",
    "        m_hat = m / (1 - beta1**t)\n",
    "        z_tilda = 1.0 / (np.sqrt(z_hat) + eps)\n",
    "\n",
    "        beta_new = beta - eta * (z_tilda * m_hat)\n",
    "\n",
    "        if np.linalg.norm(beta_new - beta) < tol:\n",
    "            beta = beta_new\n",
    "            break\n",
    "\n",
    "        beta = beta_new\n",
    "\n",
    "    return {\"beta\": beta, \"iterations\": iterations}\n",
    "\n",
    "adam = amsgrad_adam(X, y)\n",
    "print(\"tail(adam beta):\")\n",
    "print(adam[\"beta\"][-6:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abb40878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tail(gd_sto beta):\n",
      "[0.46131609 1.00191394 1.09300315 1.03833805 0.20441135 0.49217959]\n"
     ]
    }
   ],
   "source": [
    "def stochastic_gd_fixed_stepsize(X, y, beta_init=None, tol=1e-6, max_iter=10000, c=1.0, alpha=0.5, batch_size=1, seed=None):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    n, p = X.shape\n",
    "    beta = np.zeros(p) if beta_init is None else beta_init.copy()\n",
    "    iterations = 0\n",
    "\n",
    "    for it in range(1, max_iter + 1):\n",
    "        iterations += 1\n",
    "        eta = c / (it ** alpha)\n",
    "\n",
    "        batch_idx = rng.choice(n, size=batch_size, replace=False)\n",
    "        Xb = X[batch_idx, :]\n",
    "        yb = y[batch_idx]\n",
    "\n",
    "        grad = grad_f(beta, Xb, yb)\n",
    "        beta_new = beta - eta * grad\n",
    "\n",
    "        if np.linalg.norm(beta_new - beta) < tol:\n",
    "            beta = beta_new\n",
    "            break\n",
    "\n",
    "        beta = beta_new\n",
    "\n",
    "    return {\"beta\": beta, \"iterations\": iterations}\n",
    "\n",
    "gd_sto = stochastic_gd_fixed_stepsize(X, y)\n",
    "print(\"tail(gd_sto beta):\")\n",
    "print(gd_sto[\"beta\"][-6:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44ac0ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tail(gd_sto_AMS beta):\n",
      "[-0.13145157  0.18772268  0.10639134  0.25434057  0.08625003 -0.10911447]\n"
     ]
    }
   ],
   "source": [
    "def stochastic_gd_AMS(X, y, beta_init=None, tol=1e-6, max_iter=10000,\n",
    "                      batch_size=1, eta=0.01, eps=1e-8, beta1=0.9, beta2=0.999, seed=None):\n",
    "\n",
    "    rng = np.random.default_rng(seed)\n",
    "    n, p = X.shape\n",
    "    beta = np.zeros(p) if beta_init is None else beta_init.copy()\n",
    "    iterations = 0\n",
    "\n",
    "    m = np.zeros(p)\n",
    "    z = np.full(p, eps)\n",
    "    z_hat = np.full(p, eps)\n",
    "\n",
    "    for it in range(1, max_iter + 1):\n",
    "        iterations += 1\n",
    "\n",
    "        batch_idx = rng.choice(n, size=batch_size, replace=False)\n",
    "        Xb = X[batch_idx, :]\n",
    "        yb = y[batch_idx]\n",
    "\n",
    "        grad = grad_f(beta, Xb, yb)\n",
    "\n",
    "        m = beta1 * m + (1 - beta1) * grad\n",
    "        z = beta2 * z + (1 - beta2) * (grad**2)\n",
    "        z_hat = np.maximum(z_hat, z)\n",
    "\n",
    "        m_hat = m / (1 - beta1**it)\n",
    "        z_tilda = 1.0 / (np.sqrt(z_hat) + eps)\n",
    "\n",
    "        beta_new = beta - eta * (z_tilda * m_hat)\n",
    "\n",
    "        if np.linalg.norm(beta_new - beta) < tol:\n",
    "            beta = beta_new\n",
    "            break\n",
    "\n",
    "        beta = beta_new\n",
    "\n",
    "    return {\"beta\": beta, \"iterations\": iterations}\n",
    "\n",
    "gd_sto_AMS = stochastic_gd_AMS(X, y)\n",
    "print(\"tail(gd_sto_AMS beta):\")\n",
    "print(gd_sto_AMS[\"beta\"][-6:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4e8fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/t0/cl4mx7656030fy5dm2t2jwsr0000gn/T/ipykernel_35529/2326215551.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  return 1.0 / (1.0 + np.exp(-z))\n"
     ]
    }
   ],
   "source": [
    "gd_sto_100 = stochastic_gd_fixed_stepsize(X, y, batch_size=100)\n",
    "gd_sto_200 = stochastic_gd_fixed_stepsize(X, y, batch_size=200)\n",
    "gd_sto_500 = stochastic_gd_fixed_stepsize(X, y, batch_size=500)\n",
    "\n",
    "gd_sto_AMS_100 = stochastic_gd_AMS(X, y, batch_size=100)\n",
    "gd_sto_AMS_200 = stochastic_gd_AMS(X, y, batch_size=200)\n",
    "gd_sto_AMS_500 = stochastic_gd_AMS(X, y, batch_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48723aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def est_error(beta_hat, beta_ref):\n",
    "    return np.linalg.norm(beta_hat - beta_ref)\n",
    "\n",
    "error_gd_back = est_error(gd_back[\"beta\"], beta_glm)\n",
    "error_gd_nest = est_error(gd_nest[\"beta\"], beta_glm)\n",
    "error_adam = est_error(adam[\"beta\"], beta_glm)\n",
    "\n",
    "error_gd_sto_100 = est_error(gd_sto_100[\"beta\"], beta_glm)\n",
    "error_gd_sto_200 = est_error(gd_sto_200[\"beta\"], beta_glm)\n",
    "error_gd_sto_500 = est_error(gd_sto_500[\"beta\"], beta_glm)\n",
    "\n",
    "error_gd_sto_AMS_100 = est_error(gd_sto_AMS_100[\"beta\"], beta_glm)\n",
    "error_gd_sto_AMS_200 = est_error(gd_sto_AMS_200[\"beta\"], beta_glm)\n",
    "error_gd_sto_AMS_500 = est_error(gd_sto_AMS_500[\"beta\"], beta_glm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06ff02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_df = pd.DataFrame({\n",
    "    \"method\": [\n",
    "        \"gd_back\",\n",
    "        \"gd_nest\",\n",
    "        \"adam\",\n",
    "        \"gd_sto_100\",\n",
    "        \"gd_sto_200\",\n",
    "        \"gd_sto_500\",\n",
    "        \"gd_sto_AMS_100\",\n",
    "        \"gd_sto_AMS_200\",\n",
    "        \"gd_sto_AMS_500\",\n",
    "    ],\n",
    "    \"estimation_error\": [\n",
    "        error_gd_back,\n",
    "        error_gd_nest,\n",
    "        error_adam,\n",
    "        error_gd_sto_100,\n",
    "        error_gd_sto_200,\n",
    "        error_gd_sto_500,\n",
    "        error_gd_sto_AMS_100,\n",
    "        error_gd_sto_AMS_200,\n",
    "        error_gd_sto_AMS_500,\n",
    "    ],\n",
    "    \"iterations\": [\n",
    "        gd_back[\"iterations\"],\n",
    "        gd_nest[\"iterations\"],\n",
    "        adam[\"iterations\"],\n",
    "        gd_sto_100[\"iterations\"],\n",
    "        gd_sto_200[\"iterations\"],\n",
    "        gd_sto_500[\"iterations\"],\n",
    "        gd_sto_AMS_100[\"iterations\"],\n",
    "        gd_sto_AMS_200[\"iterations\"],\n",
    "        gd_sto_AMS_500[\"iterations\"],\n",
    "    ]\n",
    "})\n",
    "\n",
    "error_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
